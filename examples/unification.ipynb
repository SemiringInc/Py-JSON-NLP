{
  "cells": [
    {
      "cell_type": "markdown",
      "source": "# Token-Based Unification\n\nUnification, in its most rudimentary form, works by either merging or \noverwriting annotations, so long as their token lists align. We provide a \ncommon segmentation and tokenization strategy to all NLP-Lab packages via the \n[syntok](https://github.com/fnl/syntok) library. Let\u0027s see how all this works.\n\n## Remote Pipelines (Spacy)\n\nTo work easily with remote microservices, we will wrap them in the `RemotePipeline`\nproxy class:\n ",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "outputs": [
        {
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mBrokenPipeError\u001b[0m                           Traceback (most recent call last)",
            "\u001b[0;32m\u003cipython-input-34-b9f889c93a8f\u003e\u001b[0m in \u001b[0;36m\u003cmodule\u003e\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mspacy_pipeline\u001b[0m \u001b[0;34m\u003d\u001b[0m \u001b[0mRemotePipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\u0027https://api.linguistic.technology/spacy\u0027\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----\u003e 6\u001b[0;31m \u001b[0mspacy_json\u001b[0m \u001b[0;34m\u003d\u001b[0m \u001b[0mspacy_pipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprocess\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msample_text\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcoreferences\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdependencies\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m~/projects/venvs/nlpjson/lib/python3.6/site-packages/pyjsonnlp/pipeline.py\u001b[0m in \u001b[0;36mprocess\u001b[0;34m(self, text, coreferences, constituents, dependencies, expressions, **kwargs)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mr\u001b[0m \u001b[0;34m\u003d\u001b[0m \u001b[0mrequests\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpost\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u003d\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m!\u003d\u001b[0m \u001b[0;36m200\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstatus_code\u001b[0m \u001b[0;34m!\u003d\u001b[0m \u001b[0;36m201\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---\u003e 49\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mBrokenPipeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\u0027{r.reason} from {self.url} ({r.status_code})\u0027\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mRemotePipeline\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mBrokenPipeError\u001b[0m: Internal Server Error from https://api.linguistic.technology/spacy (500)"
          ],
          "ename": "BrokenPipeError",
          "evalue": "Internal Server Error from https://api.linguistic.technology/spacy (500)",
          "output_type": "error"
        }
      ],
      "source": "from pyjsonnlp.pipeline import RemotePipeline\n\nsample_text \u003d \"Mary asked John to buy some apples. He bought them yesterday.\"\nspacy_pipeline \u003d RemotePipeline(\u0027https://api.linguistic.technology/spacy\u0027)\n\nspacy_json \u003d spacy_pipeline.process(sample_text, coreferences\u003dTrue, dependencies\u003dTrue)",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "Here we instantiate a pipeline, and call it, requesting only coreference\nand dependency annotations. Let\u0027s look at what\u0027s inside.\n\n#### Token List",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "{\n  \"id\": 1,\n  \"text\": \"Mary\",\n  \"lemma\": \"Mary\",\n  \"xpos\": \"NNP\",\n  \"upos\": \"PROPN\",\n  \"entity_iob\": \"B\",\n  \"characterOffsetBegin\": 0,\n  \"characterOffsetEnd\": 4,\n  \"lang\": \"en\",\n  \"features\": {\n    \"Overt\": \"Yes\",\n    \"Stop\": \"No\",\n    \"Alpha\": \"Yes\",\n    \"NounType\": \"Prop\",\n    \"Number\": \"Sing\",\n    \"Foreign\": \"No\"\n  },\n  \"misc\": {\n    \"SpaceAfter\": \"Yes\"\n  },\n  \"shape\": \"Xxxx\",\n  \"entity\": \"PERSON\"\n}\n{\n  \"id\": 2,\n  \"text\": \"asked\",\n  \"lemma\": \"ask\",\n  \"xpos\": \"VBD\",\n  \"upos\": \"VERB\",\n  \"entity_iob\": \"O\",\n  \"characterOffsetBegin\": 5,\n  \"characterOffsetEnd\": 10,\n  \"lang\": \"en\",\n  \"features\": {\n    \"Overt\": \"Yes\",\n    \"Stop\": \"No\",\n    \"Alpha\": \"Yes\",\n    \"VerbForm\": \"Fin\",\n    \"Tense\": \"Past\",\n    \"Foreign\": \"No\"\n  },\n  \"misc\": {\n    \"SpaceAfter\": \"Yes\"\n  },\n  \"shape\": \"xxxx\"\n}\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "import json\n\nmary \u003d spacy_json[\u0027documents\u0027][1][\u0027tokenList\u0027][1]\nasked \u003d spacy_json[\u0027documents\u0027][1][\u0027tokenList\u0027][2]\nprint(json.dumps(mary, indent\u003d2))\nprint(json.dumps(asked, indent\u003d2))",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "#### Coreferences\nHere we see the coreference chains for these two sentences. Note \nthat tokens are indexed at the document level, and are referred \nto by their id, which is also their dictionary index.",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "[\n  {\n    \"id\": 0,\n    \"representative\": {\n      \"tokens\": [\n        3\n      ],\n      \"head\": 3\n    },\n    \"referents\": [\n      {\n        \"tokens\": [\n          9\n        ],\n        \"head\": 9\n      }\n    ]\n  },\n  {\n    \"id\": 1,\n    \"representative\": {\n      \"tokens\": [\n        6,\n        7\n      ],\n      \"head\": 7\n    },\n    \"referents\": [\n      {\n        \"tokens\": [\n          11\n        ],\n        \"head\": 11\n      }\n    ]\n  }\n]\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "coref \u003d spacy_json[\u0027documents\u0027][1][\u0027coreferences\u0027]\nprint(json.dumps(coref, indent\u003d2))\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "Let\u0027s populate some ids:\n",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "{\n  \"id\": 0,\n  \"representative\": {\n    \"tokens\": [\n      \"John\"\n    ],\n    \"head\": \"John\"\n  },\n  \"referents\": [\n    {\n      \"tokens\": [\n        \"He\"\n      ],\n      \"head\": \"He\"\n    }\n  ]\n}\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "from typing import List\nimport copy\n\ntokens \u003d spacy_json[\u0027documents\u0027][1][\u0027tokenList\u0027]\n\ndef id_list_to_text(id_list: List[int]):\n    return [tokens[t_id][\u0027text\u0027] for t_id in id_list]\n\ncoref \u003d copy.deepcopy(spacy_json[\u0027documents\u0027][1][\u0027coreferences\u0027][0])\ncoref[\u0027representative\u0027][\u0027tokens\u0027] \u003d id_list_to_text(coref[\u0027representative\u0027][\u0027tokens\u0027])\ncoref[\u0027representative\u0027][\u0027head\u0027] \u003d tokens[coref[\u0027representative\u0027][\u0027head\u0027]][\u0027text\u0027]\nfor referent in coref[\u0027referents\u0027]:\n    referent[\u0027tokens\u0027] \u003d id_list_to_text(referent[\u0027tokens\u0027])\n    referent[\u0027head\u0027] \u003d tokens[referent[\u0027head\u0027]][\u0027text\u0027]\n\nprint(json.dumps(coref, indent\u003d2))\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "In future releases this code will be encapsulated in the pyjsonnlp library.\n## Flair\nNow let\u0027s get some annotations from [Flair](https://github.com/zalandoresearch/flair).\nWe follow the same process:",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "{\n  \"id\": 1,\n  \"text\": \"Mary\",\n  \"characterOffsetBegin\": 0,\n  \"characterOffsetEnd\": 4,\n  \"features\": {\n    \"Overt\": \"Yes\"\n  },\n  \"scores\": {\n    \"upos\": 0.9905551075935364,\n    \"xpos\": 0.9998297691345215,\n    \"entity\": 0.9995647072792053\n  },\n  \"misc\": {\n    \"SpaceAfter\": \"Yes\"\n  },\n  \"upos\": \"PROPN\",\n  \"xpos\": \"NNP\",\n  \"entity\": \"S-PER\",\n  \"entity_iob\": \"B\"\n}\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "flair_pipeline \u003d RemotePipeline(\u0027https://api.linguistic.technology/flair\u0027)\nflair_json \u003d flair_pipeline.process(sample_text, expressions\u003dTrue)\nflair_mary \u003d flair_json[\u0027documents\u0027][1][\u0027tokenList\u0027][1]\nprint(json.dumps(flair_mary, indent\u003d2))",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "Let\u0027s compare Mary to Spacy:\n",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "{\n  \"id\": 1,\n  \"text\": \"Mary\",\n  \"lemma\": \"Mary\",\n  \"xpos\": \"NNP\",\n  \"upos\": \"PROPN\",\n  \"entity_iob\": \"B\",\n  \"characterOffsetBegin\": 0,\n  \"characterOffsetEnd\": 4,\n  \"lang\": \"en\",\n  \"features\": {\n    \"Overt\": \"Yes\",\n    \"Stop\": \"No\",\n    \"Alpha\": \"Yes\",\n    \"NounType\": \"Prop\",\n    \"Number\": \"Sing\",\n    \"Foreign\": \"No\"\n  },\n  \"misc\": {\n    \"SpaceAfter\": \"Yes\"\n  },\n  \"shape\": \"Xxxx\",\n  \"entity\": \"PERSON\"\n}\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "print(json.dumps(mary, indent\u003d2))",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "We see here that while most annotations align, some are present\nin Spacy only, some are present in Flair only, and some, such as `entity`,\nfollow different schemes. Scheme unification is an ongoing project for NLP-Lab.\n\nLet\u0027s see what we can do so far.\n\n## Unification\nFirst, we\u0027ll instantiate a unification object:",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "outputs": [],
      "source": "from pyjsonnlp.unification import Unifier\nunifier \u003d Unifier()",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "We have two options, `overwrite` and `add`. The difference lies in how to \nhandle keys that are present in both objects, for example `entity`. `add` will \nnot perform destructive modifications, whereas `overwrite` will. Let\u0027s start by\nperforming a non-destructive unification. We\u0027ll start by just unifying the tokens.\n### Non-destructive token unification",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "{\n  \"id\": 1,\n  \"text\": \"Mary\",\n  \"lemma\": \"Mary\",\n  \"xpos\": \"NNP\",\n  \"upos\": \"PROPN\",\n  \"entity_iob\": \"B\",\n  \"characterOffsetBegin\": 0,\n  \"characterOffsetEnd\": 4,\n  \"lang\": \"en\",\n  \"features\": {\n    \"Overt\": \"Yes\",\n    \"Stop\": \"No\",\n    \"Alpha\": \"Yes\",\n    \"NounType\": \"Prop\",\n    \"Number\": \"Sing\",\n    \"Foreign\": \"No\"\n  },\n  \"misc\": {\n    \"SpaceAfter\": \"Yes\"\n  },\n  \"shape\": \"Xxxx\",\n  \"entity\": \"PERSON\",\n  \"scores\": {\n    \"upos\": 0.9905551075935364,\n    \"xpos\": 0.9998297691345215,\n    \"entity\": 0.9995647072792053\n  }\n}\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "non_destructive \u003d unifier.add_annotation_to_a_from_b(a\u003dspacy_json, b\u003dflair_json, annotation\u003d\u0027tokens\u0027)\nnon_destructive_mary \u003d non_destructive[\u0027documents\u0027][1][\u0027tokenList\u0027][1]\nprint(json.dumps(non_destructive_mary, indent\u003d2))",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "Note that spacy\u0027s `PERSON` entity annotation is preserved, while flair\u0027s `scores`\nannotation is added. Now we\u0027ll take a look at the alternative.\n### Destructive token unification",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "{\n  \"id\": 1,\n  \"text\": \"Mary\",\n  \"lemma\": \"Mary\",\n  \"xpos\": \"NNP\",\n  \"upos\": \"PROPN\",\n  \"entity_iob\": \"B\",\n  \"characterOffsetBegin\": 0,\n  \"characterOffsetEnd\": 4,\n  \"lang\": \"en\",\n  \"features\": {\n    \"Overt\": \"Yes\",\n    \"Stop\": \"No\",\n    \"Alpha\": \"Yes\",\n    \"NounType\": \"Prop\",\n    \"Number\": \"Sing\",\n    \"Foreign\": \"No\"\n  },\n  \"misc\": {\n    \"SpaceAfter\": \"Yes\"\n  },\n  \"shape\": \"Xxxx\",\n  \"entity\": \"S-PER\",\n  \"scores\": {\n    \"upos\": 0.9905551075935364,\n    \"xpos\": 0.9998297691345215,\n    \"entity\": 0.9995647072792053\n  }\n}\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "destructive \u003d unifier.overwrite_annotation_from_a_with_b(a\u003dspacy_json, b\u003dflair_json, annotation\u003d\u0027tokens\u0027)\ndestructive_mary \u003d destructive[\u0027documents\u0027][1][\u0027tokenList\u0027][1]\nprint(json.dumps(destructive_mary, indent\u003d2))",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "Note that the `entity` field is now `S-PER`, as per flair.\n### Annotations available for unification\nWe current support:\n- tokens\n- coreference\n- expressions\n\nCoreferences are merged intelligently by matching heads.\nExpressions are either appended or overwritten entirely.\n\nLet\u0027s add flair\u0027s expressions to spacy, with a new example:\n### Expression Unification",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "[\n  {\n    \"type\": \"NP\",\n    \"scores\": {\n      \"type\": 0.8759699861208597\n    },\n    \"tokens\": [\n      3,\n      4,\n      5\n    ]\n  }\n]\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "sample_text \u003d \"John loves New York City.\"\nflair_json \u003d flair_pipeline.process(sample_text, expressions\u003dTrue)\nspacy_json \u003d spacy_pipeline.process(sample_text)\nunified \u003d unifier.overwrite_annotation_from_a_with_b(a\u003dspacy_json, b\u003dflair_json, annotation\u003d\u0027expressions\u0027)\nexpressions \u003d unified[\u0027documents\u0027][1][\u0027expressions\u0027]\nprint(json.dumps(expressions, indent\u003d2))",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "Stay tuned for more!\n",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    }
  ],
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    },
    "stem_cell": {
      "cell_type": "raw",
      "source": "",
      "metadata": {
        "pycharm": {
          "metadata": false
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}